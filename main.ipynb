{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992d3fbe",
   "metadata": {},
   "source": [
    "#### CUDA 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70d94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc506ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "학습을 진행하는 기기: cuda:0\n",
      "name NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "# GPU 확인\n",
    "\n",
    "import torch\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "print('학습을 진행하는 기기:',device)\n",
    "name = torch.cuda.get_device_name(0)\n",
    "print('name', name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9436e902",
   "metadata": {},
   "source": [
    "## Experiment options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f705aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Experiment Option\n",
    "from easydict import EasyDict\n",
    "import torch\n",
    "\n",
    "opt = EasyDict()\n",
    "opt.dataset_series = 'company'\n",
    "opt.dataset_domain = ''\n",
    "opt.subtask = 'sub1' # sub1: sentence, sub2: document(full review) only sub1\n",
    "opt.task = 'category' # category, term\n",
    "opt.num_classes = 3 # negative, positive, neutral, (+ conflict)\n",
    "opt.max_length = 200\n",
    "opt.model_name = 'kobert' # model_name: {bert_base, kobert}\n",
    "opt.pos = False # not use\n",
    "opt.lastid = False # not use\n",
    "opt.top_k = 3 # how many top-k attention score words to use\n",
    "opt.valset_ratio = 0.125\n",
    "opt.batch_size = 16\n",
    "opt.num_layers = 6 # only use bert_intermediate. how many intermediate layers to use?\n",
    "opt.num_epochs = 12\n",
    "opt.runs = 5\n",
    "opt.seed = 42\n",
    "opt.log_step = 100\n",
    "opt.patience = 5\n",
    "opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(opt.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbfadb5",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed945bf",
   "metadata": {
    "code_folding": [
     4,
     7
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train set: 2,000\n",
      "length of test set: 500\n"
     ]
    }
   ],
   "source": [
    "if opt.dataset_series == 'company':\n",
    "    path = 'dataset/{}_train.csv'.format(opt.dataset_series)\n",
    "    path_test = 'dataset/{}_test.csv'.format(opt.dataset_series)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(path)\n",
    "df_test = pd.read_csv(path_test)\n",
    "\n",
    "print('length of train set: {:,}'.format(len(df_train)))\n",
    "print('length of test set: {:,}'.format(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "287fc9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>term</th>\n",
       "      <th>category</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>높은 시장 점유율과 인지도</td>\n",
       "      <td>인지도</td>\n",
       "      <td>커리어</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>대외적으로 젊고 혁신적인 이미지</td>\n",
       "      <td>이미지</td>\n",
       "      <td>커리어</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>서비스들 배끼기에만 급급하고 스스로 혁신 하고자 하는 의지나 창의성이 전혀 없는 안...</td>\n",
       "      <td>베끼기</td>\n",
       "      <td>사내문화</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>최고의 아이티 기업으로 폭풍성장을 경험할 수 있음</td>\n",
       "      <td>폭풍성장</td>\n",
       "      <td>커리어</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>글로벌도 경험 가능</td>\n",
       "      <td>글로벌</td>\n",
       "      <td>커리어</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>업무가 많아 연장근무를 하면 한 만큼 급여를 지급하는 회사</td>\n",
       "      <td>급여</td>\n",
       "      <td>급여</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>하고싶은거 능력만 있으면 펼칠수있음 그리고 잘하면 인정도 받음</td>\n",
       "      <td>능력</td>\n",
       "      <td>커리어</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>사내 문화가 유연하고 소통이 잘 되는 편</td>\n",
       "      <td>문화</td>\n",
       "      <td>사내문화</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>업무 범위가 넓어서 역량 개발에 도움이 됨</td>\n",
       "      <td>역량</td>\n",
       "      <td>커리어</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>사내 이벤트나 복지, 사람들은 정말 좋다 휴가 쓸 땐 눈치 주지 않으며 송년회 등을...</td>\n",
       "      <td>눈치</td>\n",
       "      <td>사내문화</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  term category  \\\n",
       "0                                        높은 시장 점유율과 인지도   인지도      커리어   \n",
       "1                                     대외적으로 젊고 혁신적인 이미지   이미지      커리어   \n",
       "2     서비스들 배끼기에만 급급하고 스스로 혁신 하고자 하는 의지나 창의성이 전혀 없는 안...   베끼기     사내문화   \n",
       "3                           최고의 아이티 기업으로 폭풍성장을 경험할 수 있음  폭풍성장      커리어   \n",
       "4                                            글로벌도 경험 가능   글로벌      커리어   \n",
       "...                                                 ...   ...      ...   \n",
       "1995                   업무가 많아 연장근무를 하면 한 만큼 급여를 지급하는 회사    급여       급여   \n",
       "1996                 하고싶은거 능력만 있으면 펼칠수있음 그리고 잘하면 인정도 받음    능력      커리어   \n",
       "1997                             사내 문화가 유연하고 소통이 잘 되는 편    문화     사내문화   \n",
       "1998                            업무 범위가 넓어서 역량 개발에 도움이 됨    역량      커리어   \n",
       "1999  사내 이벤트나 복지, 사람들은 정말 좋다 휴가 쓸 땐 눈치 주지 않으며 송년회 등을...    눈치     사내문화   \n",
       "\n",
       "      polarity  \n",
       "0     positive  \n",
       "1     positive  \n",
       "2     negative  \n",
       "3     positive  \n",
       "4     positive  \n",
       "...        ...  \n",
       "1995  positive  \n",
       "1996  positive  \n",
       "1997  positive  \n",
       "1998  positive  \n",
       "1999  positive  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ebe1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = df_train.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd436f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import clean_sentence, preprocess\n",
    "df_train = clean_sentence(df=df_train, clean_func=preprocess)\n",
    "df_test = clean_sentence(df=df_test, clean_func=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace04058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBertTokenizer\n",
    "\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baafbe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: True\n",
      "2,000 samples in this dataset\n",
      "category: True\n",
      "500 samples in this dataset\n"
     ]
    }
   ],
   "source": [
    "from data_utils import Category_Classification_Dataset as Dataset\n",
    "\n",
    "trainset = Dataset(df=df_train, tokenizer=tokenizer, opt=opt, pos_encoding=False)\n",
    "testset = Dataset(df=df_test, tokenizer=tokenizer, opt=opt, pos_encoding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bca2a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of datasets 1750 : 250 : 500\n"
     ]
    }
   ],
   "source": [
    "from data_utils import custom_random_split as rs\n",
    "\n",
    "train_set, val_set, test_set = rs(dataset=trainset, testset=testset,\n",
    "                                  val_ratio=opt.valset_ratio, random_seed=opt.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f712d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=opt.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=opt.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=opt.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa50e8",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4a510",
   "metadata": {},
   "source": [
    "use top-k attention words + some tokens + pooling\n",
    "\n",
    "- top-k: 3, 4\n",
    "- additional tokens: [SEP_1], [SEP_2], both [SEP], [CLS], pair words(aspect words)\n",
    "- pooling: 'mean' or 'bi-gru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6e99bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kobert'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "865d39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.kobert import *\n",
    "\n",
    "if opt.model_name == 'kobert':\n",
    "    model = KoBERT(opt=opt, embed_dim=768, fc_hid_dim=128, top_k=opt.top_k, att_head='all', att_pooling='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "215b2a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92,189,187 total parameters in this model\n",
      "92,189,187 trainable parameters in this model\n"
     ]
    }
   ],
   "source": [
    "from models.parameters import get_parameters\n",
    "total, params = get_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d7485",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "151fc3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> RUN NUMBER: 01 <<<<<\n",
      "   global step: 100 | train loss: 0.531, train_acc: 73.88%\n",
      "Epoch: 01 | Val Loss: 0.028 | Val Acc: 83.20%\n",
      "   global step: 200 | train loss: 0.272, train_acc: 89.79%\n",
      "Epoch: 02 | Val Loss: 0.018 | Val Acc: 89.20%\n",
      "   global step: 300 | train loss: 0.134, train_acc: 95.78%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_3_val_acc_84.8%\n",
      "Epoch: 03 | Val Loss: 0.026 | Val Acc: 84.80%\n",
      "   global step: 400 | train loss: 0.097, train_acc: 96.70%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_4_val_acc_89.2%\n",
      "Epoch: 04 | Val Loss: 0.020 | Val Acc: 89.20%\n",
      "   global step: 500 | train loss: 0.066, train_acc: 98.23%\n",
      "Epoch: 05 | Val Loss: 0.028 | Val Acc: 88.00%\n",
      "   global step: 600 | train loss: 0.043, train_acc: 99.00%\n",
      "Epoch: 06 | Val Loss: 0.039 | Val Acc: 84.40%\n",
      "   global step: 700 | train loss: 0.055, train_acc: 98.44%\n",
      "Epoch: 07 | Val Loss: 0.026 | Val Acc: 86.40%\n",
      "   global step: 800 | train loss: 0.032, train_acc: 99.38%\n",
      "Epoch: 08 | Val Loss: 0.023 | Val Acc: 88.80%\n",
      "   global step: 900 | train loss: 0.025, train_acc: 99.38%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_9_val_acc_90.8%\n",
      "Epoch: 09 | Val Loss: 0.024 | Val Acc: 90.80%\n",
      "   global step: 1,000 | train loss: 0.048, train_acc: 98.75%\n",
      "   global step: 1,100 | train loss: 0.032, train_acc: 99.37%\n",
      "Epoch: 10 | Val Loss: 0.022 | Val Acc: 90.40%\n",
      "   global step: 1,200 | train loss: 0.017, train_acc: 99.62%\n",
      "Epoch: 11 | Val Loss: 0.035 | Val Acc: 88.40%\n",
      "   global step: 1,300 | train loss: 0.030, train_acc: 99.10%\n",
      "Epoch: 12 | Val Loss: 0.025 | Val Acc: 90.00%\n",
      "Best Val Acc: 90.80% at 9 epoch\n",
      ">> saved best state dict: state_dict/BEST_kobert_company_preprocess5_val_acc_90.8%\n",
      "RUN: 01 | Test loss: 0.035 | Test_acc: 88.40% | Test_f1: 88.37\n",
      ">>>>> RUN 01 HAS BEEN FINISHED <<<<<\n",
      ">>>>> RUN NUMBER: 02 <<<<<\n",
      "   global step: 100 | train loss: 0.519, train_acc: 75.06%\n",
      "Epoch: 01 | Val Loss: 0.020 | Val Acc: 85.60%\n",
      "   global step: 200 | train loss: 0.244, train_acc: 90.49%\n",
      "Epoch: 02 | Val Loss: 0.019 | Val Acc: 88.40%\n",
      "   global step: 300 | train loss: 0.126, train_acc: 95.47%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_3_val_acc_87.2%\n",
      "Epoch: 03 | Val Loss: 0.024 | Val Acc: 87.20%\n",
      "   global step: 400 | train loss: 0.069, train_acc: 98.57%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_4_val_acc_88.4%\n",
      "Epoch: 04 | Val Loss: 0.020 | Val Acc: 88.40%\n",
      "   global step: 500 | train loss: 0.049, train_acc: 98.65%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_5_val_acc_90.8%\n",
      "Epoch: 05 | Val Loss: 0.022 | Val Acc: 90.80%\n",
      "   global step: 600 | train loss: 0.034, train_acc: 99.12%\n",
      "Epoch: 06 | Val Loss: 0.028 | Val Acc: 86.40%\n",
      "   global step: 700 | train loss: 0.082, train_acc: 97.03%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_7_val_acc_92.8%\n",
      "Epoch: 07 | Val Loss: 0.019 | Val Acc: 92.80%\n",
      "   global step: 800 | train loss: 0.009, train_acc: 100.00%\n",
      "Epoch: 08 | Val Loss: 0.023 | Val Acc: 90.80%\n",
      "   global step: 900 | train loss: 0.007, train_acc: 100.00%\n",
      "Epoch: 09 | Val Loss: 0.019 | Val Acc: 91.20%\n",
      "   global step: 1,000 | train loss: 0.038, train_acc: 99.38%\n",
      "   global step: 1,100 | train loss: 0.029, train_acc: 99.14%\n",
      "Epoch: 10 | Val Loss: 0.026 | Val Acc: 90.40%\n",
      "   global step: 1,200 | train loss: 0.010, train_acc: 99.88%\n",
      "Epoch: 11 | Val Loss: 0.025 | Val Acc: 91.60%\n",
      "   global step: 1,300 | train loss: 0.026, train_acc: 99.38%\n",
      ">> early stop\n",
      "Best Val Acc: 92.80% at 7 epoch\n",
      ">> saved best state dict: state_dict/BEST_kobert_company_preprocess5_val_acc_92.8%\n",
      "RUN: 02 | Test loss: 0.034 | Test_acc: 88.20% | Test_f1: 88.17\n",
      ">>>>> RUN 02 HAS BEEN FINISHED <<<<<\n",
      ">>>>> RUN NUMBER: 03 <<<<<\n",
      "   global step: 100 | train loss: 0.607, train_acc: 69.62%\n",
      "Epoch: 01 | Val Loss: 0.030 | Val Acc: 80.00%\n",
      "   global step: 200 | train loss: 0.286, train_acc: 88.26%\n",
      "Epoch: 02 | Val Loss: 0.017 | Val Acc: 89.20%\n",
      "   global step: 300 | train loss: 0.159, train_acc: 95.00%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_3_val_acc_88.4%\n",
      "Epoch: 03 | Val Loss: 0.018 | Val Acc: 88.40%\n",
      "   global step: 400 | train loss: 0.088, train_acc: 97.32%\n",
      "Epoch: 04 | Val Loss: 0.022 | Val Acc: 86.80%\n",
      "   global step: 500 | train loss: 0.064, train_acc: 98.44%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_5_val_acc_89.6%\n",
      "Epoch: 05 | Val Loss: 0.020 | Val Acc: 89.60%\n",
      "   global step: 600 | train loss: 0.041, train_acc: 99.25%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_6_val_acc_90.8%\n",
      "Epoch: 06 | Val Loss: 0.019 | Val Acc: 90.80%\n",
      "   global step: 700 | train loss: 0.020, train_acc: 99.53%\n",
      "Epoch: 07 | Val Loss: 0.022 | Val Acc: 89.20%\n",
      "   global step: 800 | train loss: 0.030, train_acc: 99.38%\n",
      "Epoch: 08 | Val Loss: 0.027 | Val Acc: 90.80%\n",
      "   global step: 900 | train loss: 0.026, train_acc: 99.69%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_9_val_acc_92.4%\n",
      "Epoch: 09 | Val Loss: 0.025 | Val Acc: 92.40%\n",
      "   global step: 1,000 | train loss: 0.008, train_acc: 100.00%\n",
      "   global step: 1,100 | train loss: 0.031, train_acc: 99.20%\n",
      "Epoch: 10 | Val Loss: 0.024 | Val Acc: 91.60%\n",
      "   global step: 1,200 | train loss: 0.045, train_acc: 98.75%\n",
      "Epoch: 11 | Val Loss: 0.028 | Val Acc: 88.40%\n",
      "   global step: 1,300 | train loss: 0.007, train_acc: 99.93%\n",
      "Epoch: 12 | Val Loss: 0.027 | Val Acc: 90.00%\n",
      "Best Val Acc: 92.40% at 9 epoch\n",
      ">> saved best state dict: state_dict/BEST_kobert_company_preprocess5_val_acc_92.4%\n",
      "RUN: 03 | Test loss: 0.031 | Test_acc: 89.80% | Test_f1: 89.78\n",
      ">>>>> RUN 03 HAS BEEN FINISHED <<<<<\n",
      ">>>>> RUN NUMBER: 04 <<<<<\n",
      "   global step: 100 | train loss: 0.499, train_acc: 76.19%\n",
      "Epoch: 01 | Val Loss: 0.025 | Val Acc: 84.40%\n",
      "   global step: 200 | train loss: 0.241, train_acc: 91.32%\n",
      "Epoch: 02 | Val Loss: 0.019 | Val Acc: 87.60%\n",
      "   global step: 300 | train loss: 0.131, train_acc: 96.17%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_3_val_acc_85.2%\n",
      "Epoch: 03 | Val Loss: 0.027 | Val Acc: 85.20%\n",
      "   global step: 400 | train loss: 0.121, train_acc: 95.98%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_4_val_acc_91.2%\n",
      "Epoch: 04 | Val Loss: 0.020 | Val Acc: 91.20%\n",
      "   global step: 500 | train loss: 0.071, train_acc: 98.02%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_5_val_acc_92.0%\n",
      "Epoch: 05 | Val Loss: 0.018 | Val Acc: 92.00%\n",
      "   global step: 600 | train loss: 0.052, train_acc: 98.50%\n",
      "Epoch: 06 | Val Loss: 0.023 | Val Acc: 89.60%\n",
      "   global step: 700 | train loss: 0.069, train_acc: 98.59%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_7_val_acc_93.6%\n",
      "Epoch: 07 | Val Loss: 0.021 | Val Acc: 93.60%\n",
      "   global step: 800 | train loss: 0.041, train_acc: 98.75%\n",
      "Epoch: 08 | Val Loss: 0.021 | Val Acc: 88.80%\n",
      "   global step: 900 | train loss: 0.028, train_acc: 99.06%\n",
      "Epoch: 09 | Val Loss: 0.025 | Val Acc: 91.20%\n",
      "   global step: 1,000 | train loss: 0.011, train_acc: 99.38%\n",
      "   global step: 1,100 | train loss: 0.019, train_acc: 99.43%\n",
      "Epoch: 10 | Val Loss: 0.021 | Val Acc: 92.40%\n",
      "   global step: 1,200 | train loss: 0.025, train_acc: 99.31%\n",
      "Epoch: 11 | Val Loss: 0.023 | Val Acc: 90.80%\n",
      "   global step: 1,300 | train loss: 0.009, train_acc: 99.79%\n",
      ">> early stop\n",
      "Best Val Acc: 93.60% at 7 epoch\n",
      ">> saved best state dict: state_dict/BEST_kobert_company_preprocess5_val_acc_93.6%\n",
      "RUN: 04 | Test loss: 0.029 | Test_acc: 91.40% | Test_f1: 91.40\n",
      ">>>>> RUN 04 HAS BEEN FINISHED <<<<<\n",
      ">>>>> RUN NUMBER: 05 <<<<<\n",
      "   global step: 100 | train loss: 0.449, train_acc: 79.94%\n",
      "Epoch: 01 | Val Loss: 0.024 | Val Acc: 87.20%\n",
      "   global step: 200 | train loss: 0.169, train_acc: 95.07%\n",
      "Epoch: 02 | Val Loss: 0.016 | Val Acc: 90.40%\n",
      "   global step: 300 | train loss: 0.078, train_acc: 97.73%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_3_val_acc_91.2%\n",
      "Epoch: 03 | Val Loss: 0.016 | Val Acc: 91.20%\n",
      "   global step: 400 | train loss: 0.051, train_acc: 98.66%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_4_val_acc_92.8%\n",
      "Epoch: 04 | Val Loss: 0.017 | Val Acc: 92.80%\n",
      "   global step: 500 | train loss: 0.039, train_acc: 98.65%\n",
      "Epoch: 05 | Val Loss: 0.018 | Val Acc: 92.80%\n",
      "   global step: 600 | train loss: 0.022, train_acc: 99.38%\n",
      "Epoch: 06 | Val Loss: 0.017 | Val Acc: 91.60%\n",
      "   global step: 700 | train loss: 0.017, train_acc: 99.69%\n",
      ">> saved: state_dict/kobert_company_preprocess5_epoch_7_val_acc_94.0%\n",
      "Epoch: 07 | Val Loss: 0.016 | Val Acc: 94.00%\n",
      "   global step: 800 | train loss: 0.031, train_acc: 99.17%\n",
      "Epoch: 08 | Val Loss: 0.025 | Val Acc: 91.20%\n",
      "   global step: 900 | train loss: 0.054, train_acc: 98.75%\n",
      "Epoch: 09 | Val Loss: 0.017 | Val Acc: 92.40%\n",
      "   global step: 1,000 | train loss: 0.024, train_acc: 98.75%\n",
      "   global step: 1,100 | train loss: 0.016, train_acc: 99.37%\n",
      "Epoch: 10 | Val Loss: 0.015 | Val Acc: 93.20%\n",
      "   global step: 1,200 | train loss: 0.014, train_acc: 99.69%\n",
      "Epoch: 11 | Val Loss: 0.018 | Val Acc: 92.80%\n",
      "   global step: 1,300 | train loss: 0.008, train_acc: 99.86%\n",
      ">> early stop\n",
      "Best Val Acc: 94.00% at 7 epoch\n",
      ">> saved best state dict: state_dict/BEST_kobert_company_preprocess5_val_acc_94.0%\n",
      "RUN: 05 | Test loss: 0.017 | Test_acc: 94.80% | Test_f1: 94.79\n",
      ">>>>> RUN 05 HAS BEEN FINISHED <<<<<\n",
      "Sheets has been exported\n",
      "====================\n",
      "best run: 05 | best acc: 94.80 | best f1: 94.79 | best run path: state_dict/BEST_kobert_company_preprocess5_val_acc_94.0%\n",
      "<<<Averages>>>\n",
      "test_loss: 0.029 | test_acc: 90.52 | test_f1: 90.50\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from custom_trainer import *\n",
    "\n",
    "optimizer = optim.AdamW(params, lr=2e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.8) # can't use for multiple runs\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "result_dict, best_path = runs(trainer=trainer, train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "                             model=model, criterion=criterion, optimizer=optimizer, scheduler=False, opt=opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
